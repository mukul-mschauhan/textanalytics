{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction to Natural Language Processing\n\nNatural Language Processing is a set of Techniques that are used to analyse the text data and also help machines learn from the text. NLTK is the name of the library that is the most common name in the world of Text Analytics or NLP.\n\nIn this tutorial, we would look into tweets and I will be using a simple method from scikit learn to develop a machine learning model."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Dictionary\n\n1. text - It represents the text of the tweet\n\n2. keyword - A \"Particular Word\" from that tweet (although this may be blank!)\nAbout Keyword: Keyword targeting allows you to connect with users based on words and phrases they've recently Tweeted or searched for on Twitter. This marketing capability allows you to reach your target audience when your business is most relevant to them.\n\n3. Location - Location of the Tweet\n\n4. Target - You are predicting whether a given tweet is about a real disaster or not. 1 rep Disaster Tweet and 0 Represents Not a Disaster"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets analyse what tweets are not disaster tweets\ntrain.loc[train.target==0,\"text\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets Analyze Disaster Tweets\ntrain.loc[train.target==1,\"text\"].values[1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Count Vectorizer\nNow since we know that the whether a tweet is disaster or not, depends on the Words used in the tweet. Hence, we will know make the count of the words that can be easily fed into the machine learning model at a later stage.\n\nHere, we will be using the CountVectorizer from Scikit Learn to do the job.\n\nLets see an example of the Count Vectorizer before we put it to use.\n\n#### Explanation of Count Vectorizer\nThe CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n\nYou can use it as follows:\n\n1. Create an instance of the CountVectorizer class.\n2. Call the fit() function in order to learn a vocabulary from one or more documents.\n3. Call the transform() function on one or more documents as needed to encode each as a vector.\n\nAn encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document.\n\nInformation Source: https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Count Vectorizer...\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vectorizer = CountVectorizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example of Count Vectorizer\ntext = [\"Jack and Jill went up the Hill!\"]\ncount_vectorizer.fit(text) # fit function helps learn a vocabulary about the text\ntransformed = count_vectorizer.transform(text) # encodes the text/doc into a vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The vectors returned from a call to transform() will be sparse vectors, and you can transform them back to numpy arrays to look and better understand what is going on by calling the toarray() function."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(transformed.shape)\nprint(type(transformed))\nprint(transformed.toarray())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to understand what has been transformed, we can call vocabulary"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(count_vectorizer.vocabulary_)\n\n# Observations: Punctuations are ignored and all words are converted into Lower Case","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test with another word\nprint(count_vectorizer.transform([\"Jack\"]).toarray()) # able to recognize the word in upper case | Location is 2\nprint(count_vectorizer.transform([\"and\"]).toarray()) # Loc is 0 as per above vocabulary\nprint(count_vectorizer.transform([\"Jill\"]).toarray())\nprint(count_vectorizer.transform([\"Mukul Singh\"]).toarray()) # No words found and hence all 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets get the count of first 5 tweets\nexmple  = count_vectorizer.fit_transform(train[\"text\"][0:5])\n\nprint(exmple[0].todense().shape)\nprint(exmple[0].todense())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. This means that there 54 tokens/unique words in the first 5 tweets\n2. The first vector has been printed for the first tweet."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(count_vectorizer.vocabulary_))\nprint(\"Unique Words are: \")\nprint(np.unique(list(count_vectorizer.vocabulary_)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets Create a Vector for all the tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train Set\nalltweets = count_vectorizer.fit_transform(train[\"text\"]) # Transformed the Train Tweets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, since we want to map those tweets/words in test set which are there in train and hence, we will not be using fit transform, instead we will use transform only to do the job."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test Set\ntesttweets = count_vectorizer.transform(test[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modelling\n\nNow we will be starting with the model. Since, we have a target variable that has 0 and 1 meaning it is a classification problem and hence we will be using a RidgeClassifier along with Random Forest and Gradient Boosting Model as a base ensemble model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RidgeClassifier\nfrom sklearn.model_selection import cross_val_score\nridge = RidgeClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cross_val_score(ridge, alltweets, train.target, cv = 5, scoring =\"f1\").mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\ngbm = GradientBoostingClassifier()\nrf = RandomForestClassifier()\nvc = VotingClassifier(estimators = [(\"rf\", rf), (\"ridge\", ridge), (\"GBM\", gbm)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vc.fit(alltweets, train.target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"solution = pd.DataFrame({\"id\": test.id, \"target\": vc.predict(testtweets)})\nsolution.to_csv(\"VC Model.csv\", index=False) # Kaggle: 0.78016","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, Since we see that the F1 score comes out to be 0.7816 which is a good start for the modelling. Having said that, we can try different models such as Neural Net, LSTM, TFIDF etc to improve the accuracy of the model.\n\nThe work is inspired from https://www.kaggle.com/philculliton/nlp-getting-started-tutorial"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}